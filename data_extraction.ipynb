{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract player info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as url\n",
    "import re\n",
    "\n",
    "def find_player_id_per_year(years):\n",
    "    players_season = {}\n",
    "    for year in years:\n",
    "        players = {}\n",
    "        url_year = \"http://www.espn.com/nba/statistics/player/_/stat/scoring-per-game/sort/avgPoints/year/\" + str(year) + \"/qualified/false\"\n",
    "        for page in [-1] + list(range(41, 482, 40)):\n",
    "            url_page = \"\"\n",
    "            if page == -1:\n",
    "                url_page = url_year\n",
    "            else:\n",
    "                url_page = url_year + \"/count/\" + str(page)\n",
    "            html_page = url.urlopen(url_page)\n",
    "            soup = BeautifulSoup(html_page)\n",
    "            for link in soup.findAll('a', attrs={'href': re.compile(\"^http://www.espn.com/nba/player/_/id\")}):\n",
    "                players_id = link.get('href').split('/')\n",
    "                players[players_id[len(players_id)-1]] = players_id[len(players_id)-2]\n",
    "        players_season[str(year-1) + \"-\" + str(year)] = players\n",
    "    return players_season;    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "def read_players(player_id_years, years):\n",
    "    player_info = {}\n",
    "    for year in years:\n",
    "        player_id = player_id_years[str(year-1)+\"-\"+str(year)]\n",
    "        for player in player_id:\n",
    "            url_path = \"http://www.espn.com/nba/player/gamelog/_/id/\" + player_id[player] + \"/year/\" + str(year) + \"/\" + player\n",
    "            try:\n",
    "                dataframe = pandas.read_html(url_path)\n",
    "                if player not in player_info:\n",
    "                    player_info[player] = {}\n",
    "                player_info[player][str(year-1)+\"-\"+str(year)] = dataframe\n",
    "            except url.HTTPError:\n",
    "                pass\n",
    "    return player_info\n",
    "# http://www.espn.com/nba/player/stats/_/id/2994526/bryn-forbes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_id_years = find_player_id_per_year([2018])\n",
    "test_player_info = read_players(player_id_years,[2018])\n",
    "player_id_years = find_player_id_per_year([2013,2014,2015,2016,2017])\n",
    "train_player_info = read_players(player_id_years, [2013,2014,2015,2016,2017])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('player_data/test_player_info.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_player_info, fp)\n",
    "with open('player_data/train_player_info.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_player_info, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_data_processing(data):\n",
    "    X = {}\n",
    "    # print (data[player][2017][1][index:index+2])\n",
    "    for player in data:\n",
    "        for year in data[player]:\n",
    "            feature  = []\n",
    "            table = np.array(data[player][year][1][0])\n",
    "            if len(table)<4:\n",
    "                continue\n",
    "    #         print (table)\n",
    "            if 'REGULAR SEASON STATS' not in table:\n",
    "                continue\n",
    "            index = int(np.argwhere(table == 'REGULAR SEASON STATS'))\n",
    "    #         print (index)\n",
    "            for i in range (1,15):\n",
    "                x= str(data[player][year][1][index+1:index+2][i])\n",
    "                x = x.split('\\n')[0]\n",
    "                x = x.split(' ')[-1]\n",
    "                if '-' in x:\n",
    "                    x = x.split('-')[-1]\n",
    "                feature.append(x)\n",
    "            feature = np.asarray(feature)\n",
    "            feature = list(map(eval, feature))\n",
    "            if player not in X:\n",
    "                X[player]={}\n",
    "            if year not in X[player]:\n",
    "                X[player][year] = []\n",
    "            X[player][year] = feature\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('player_data/test_player_info.pkl', 'rb') as fp:\n",
    "    test_player_info = pickle.load(fp)\n",
    "with open('player_data/train_player_info.pkl', 'rb') as fp:\n",
    "    train_player_info = pickle.load(fp)\n",
    "    \n",
    "test_processed_player_data = player_data_processing(test_player_info)\n",
    "train_processed_player_data = player_data_processing(train_player_info)\n",
    "\n",
    "with open('player_data/test_processed_player_data.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_processed_player_data, fp)\n",
    "with open('player_data/train_processed_player_data.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_processed_player_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_processed_player_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_processed_player_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract team info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_teams_url(team_url):\n",
    "    html_page = url.urlopen(team_url)\n",
    "    soup = BeautifulSoup(html_page)\n",
    "    team_url = []\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^http://www.espn.com/nba/team/_/name/\")}):\n",
    "        team_url.append(link.get('href'))\n",
    "    return set(team_url)\n",
    "\n",
    "\n",
    "def read_teams(team_url, years):\n",
    "    team_stat = {}\n",
    "    for url_path in team_url:\n",
    "        url_component = url_path.split('/_/')\n",
    "        stat_url = url_component[0] + '/schedule/_/' + 'name/' + url_component[1].split('/')[1] + '/season/'\n",
    "        year_stat = {}\n",
    "        for year in years:\n",
    "            try:\n",
    "                dataframe = pandas.read_html(stat_url + str(year) + '/seasontype/2')\n",
    "                year_stat[year] = dataframe\n",
    "            except url.HTTPError:\n",
    "                pass\n",
    "            print(stat_url + str(year) + '/seasontype/2')\n",
    "        frames = []\n",
    "        for year in years:\n",
    "            df_changed = year_stat[year][2].loc[2:, 0:2]\n",
    "            before_half = df_changed[0].str.split(\" \",expand=True)[1].isin([\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\"])\n",
    "            df_changed[0][before_half] = df_changed[0][before_half]+', '+str(year)\n",
    "            df_changed[0][~before_half] = df_changed[0][~before_half]+', '+str(year-1)\n",
    "            df_changed[3] = str(year - 1) + \"-\" + str(year)\n",
    "            frames.append(df_changed)\n",
    "        result = pandas.concat(frames)\n",
    "        result.index = range(len(result))\n",
    "        \n",
    "        team_name_component = url_component[1][5:].split('/')[1].split('-')[:-1]\n",
    "        team_name = ' '.join(team_name_component)\n",
    "        \n",
    "        team_stat[team_name] = result\n",
    "    return team_stat\n",
    "\n",
    "def read_teams_total_and_players_per_year(team_url, years):\n",
    "    team_total = {}\n",
    "    for url_path in team_url:\n",
    "        url_component = url_path.split('/_/')\n",
    "        stat_url = url_component[0] + '/stats/_/' + 'name/' + url_component[1].split('/')[1] + '/year/'\n",
    "        year_info = {}\n",
    "        for year in years:\n",
    "            try:\n",
    "                dataframe = pandas.read_html(stat_url + str(year))\n",
    "                team_total_info = pandas.concat([dataframe[0][len(dataframe[0])-1:],dataframe[1][len(dataframe[1])-1:]])\n",
    "                team_total_info.index = range(len(team_total_info))\n",
    "                team_players = dataframe[0][2:][0]\n",
    "                team_players.index = range(len(team_players))\n",
    "                year_info[str(year-1) + \"-\" + str(year)] = (team_total_info, team_players)\n",
    "            except url.HTTPError:\n",
    "                pass\n",
    "            print(stat_url + str(year))\n",
    "        \n",
    "        team_name_component = url_component[1][5:].split('/')[1].split('-')[:-1]\n",
    "        team_name = ' '.join(team_name_component)\n",
    "        \n",
    "        team_total[team_name] = year_info\n",
    "    return team_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "team_url = find_teams_url(\"http://www.espn.com/nba/teams\")\n",
    "# test = read_teams(team_url, [2018])\n",
    "# train = read_teams(team_url, [2013,2014,2015,2016,2017])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "def data_processing(test):\n",
    "    game_info = {}\n",
    "    for team in test:\n",
    "        for i in range(5,len(test[team][1])):\n",
    "            #opponent of the current game\n",
    "            if team not in game_info:\n",
    "                game_info[team] = []\n",
    "            #the results of former 5 games of the team\n",
    "            info = []\n",
    "            #opponent info\n",
    "            opp_info = test[team][1][i].strip().lower()\n",
    "            if opp_info[0] == '@':\n",
    "                opp = opp_info[1:].strip()\n",
    "                info.append(opp)\n",
    "            else:\n",
    "                opp = opp_info[2:].strip()\n",
    "                info.append(opp)\n",
    "            #lose or win\n",
    "            for j in range(1,6):\n",
    "                if test[team][2][5-j][0] == 'L':\n",
    "                    info.append('0')\n",
    "                else:\n",
    "                    info.append('1')\n",
    "            #Home field or away of the current game\n",
    "            if (test[team][1][i].strip()[0] == 'v'):\n",
    "                info.append('1')\n",
    "            else:\n",
    "                info.append('0')\n",
    "#             #back to back\n",
    "#             print(team)\n",
    "#             d1 = datetime.strptime(test[team][0][i], \"%a, %b %d, %Y\")\n",
    "#             d2 = datetime.strptime(test[team][0][i-1], \"%a, %b %d, %Y\")\n",
    "#             if (d1-d2).days==1:\n",
    "#                 info.append('1')\n",
    "#             else:\n",
    "#                 info.append('0')\n",
    "            #the feature vector and result of the current game       \n",
    "            if (test[team][2][i][0] == 'L'):\n",
    "                game_info[team].append((info, '0', test[team][3][i]))\n",
    "            else:\n",
    "                game_info[team].append((info, '1', test[team][3][i]))\n",
    "                \n",
    "    return game_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_and_players_to_dict(data):\n",
    "    total_dict = {}\n",
    "    team_players = {}\n",
    "    for team in data:\n",
    "        total_dict[team] = {}\n",
    "        team_players[team] = {}\n",
    "        for season in data[team]:\n",
    "            #process team total info first\n",
    "            lst = [data[team][season][0][1][0]]\n",
    "            for i in range(4, 14):\n",
    "                lst.append(str(data[team][season][0][i][0]))\n",
    "            for i in range(1, 15):\n",
    "                lst.append(str(data[team][season][0][i][1]))\n",
    "            total_dict[team][season] = lst\n",
    "            #determine the players of each team per season\n",
    "            team_players[team][season] = [data[team][season][1][i][:data[team][season][1][i].rfind(',')] for i in range(len(data[team][season][1]))]\n",
    "    return total_dict, team_players\n",
    "\n",
    "test_total_and_players = read_teams_total_and_players_per_year(team_url, [2018])\n",
    "train_total_and_players = read_teams_total_and_players_per_year(team_url, [2013,2014,2015,2016,2017])\n",
    "\n",
    "test_total, test_team_players = total_and_players_to_dict(test_total_and_players)\n",
    "train_total, train_team_players = total_and_players_to_dict(train_total_and_players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('team_data/test_total.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_total, fp)\n",
    "with open('team_data/train_total.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_total, fp)\n",
    "\n",
    "team_players = {}\n",
    "for team in train_team_players:\n",
    "    team_players[team] = train_team_players[team]\n",
    "    for season in test_team_players[team]:\n",
    "        team_players[team][season] = test_team_players[team][season]\n",
    "    \n",
    "with open('team_data/team_players.pkl', 'wb') as fp:\n",
    "    pickle.dump(team_players, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(team_players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "all_games_train = data_processing(train)\n",
    "all_games_test = data_processing(test)\n",
    "\n",
    "with open('team_data/all_games_train.pkl', 'wb') as fp:\n",
    "    pickle.dump(all_games_train, fp)\n",
    "with open('team_data/all_games_test.pkl', 'wb') as fp:\n",
    "    pickle.dump(all_games_train, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract game info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_game_info(team_url, years):\n",
    "    games_url = {}\n",
    "    for url_path in team_url:\n",
    "        url_component = url_path.split('/_/')\n",
    "        stat_url = url_component[0] + '/schedule/_/' + 'name/' + url_component[1].split('/')[1] + '/season/'\n",
    "        for year in years:\n",
    "            if year not in games_url:\n",
    "                games_url[year] = set()\n",
    "            try:\n",
    "                html_page = url.urlopen(stat_url + str(year) + '/seasontype/2')\n",
    "                soup = BeautifulSoup(html_page)\n",
    "                for link in soup.findAll('a', attrs={'href': re.compile(\"^http://www.espn.com/nba/game\")}):\n",
    "                    games_url[year].add(\"http://www.espn.com/nba/boxscore?gameId=\" + link.get('href').split('=')[-1])\n",
    "            except url.HTTPError:\n",
    "                pass\n",
    "    games_info = {}\n",
    "    for year in games_url:\n",
    "        games_info[str(year-1)+\"-\"+str(year)] = []\n",
    "        for url_path in games_url[year]:\n",
    "            try:\n",
    "                dataframe = pandas.read_html(url_path)\n",
    "                games_info[str(year-1)+\"-\"+str(year)].append(dataframe)\n",
    "            except url.HTTPError:\n",
    "                pass\n",
    "        \n",
    "    return games_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_info = read_game_info(team_url, [2018])\n",
    "with open('game_data/test_game_info_raw.pkl', 'wb') as fp:\n",
    "    pickle.dump(game_info, fp)\n",
    "\n",
    "train_game_info = read_game_info(team_url, [2013,2014,2015,2016,2017])\n",
    "with open('game_data/train_game_info_raw.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_game_info, fp)\n",
    "#example: http://www.espn.com/nba/boxscore?gameId=400974442"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('game_data/test_game_info_raw.pkl', 'rb') as fp:\n",
    "    test_game_info = pickle.load(fp)\n",
    "with open('game_data/train_game_info_raw.pkl', 'rb') as fp:\n",
    "    train_game_info = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = test_game_info[\"2017-2018\"][0][1]\n",
    "# df.MIN = pandas.to_numeric(df.MIN, errors='coerce')\n",
    "# df = df.sort_values(by=['MIN'], ascending=False)\n",
    "# test_game_info[\"2017-2018\"][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_game_info(data, team_player, team_name_brev):\n",
    "    games_per_season = {}\n",
    "    for season in data:\n",
    "        games_per_season[season] = []\n",
    "        for df in data[season]:\n",
    "            scores = tuple(df[0]['T'])\n",
    "            #team1 info\n",
    "            team1 = team_name_brev[df[0]['Unnamed: 0'][0]]\n",
    "            df[1].MIN = pandas.to_numeric(df[1].MIN, errors='coerce')\n",
    "            starters = [cleanName(name, team_player[team1]) for name in df[1].sort_values(by=['MIN'], ascending=False)[:5]]\n",
    "            ben = [cleanName(name, team_player[team1]) for name in df[1].sort_values(by=['MIN'], ascending=False)[5:10]]\n",
    "            team1_players = starters+ben\n",
    "            #team2 info\n",
    "            team2 = team_name_brev[df[0]['Unnamed: 0'][1]]\n",
    "            df[2].MIN = pandas.to_numeric(df[2].MIN, errors='coerce')\n",
    "            starters = [cleanName(name, team_player[team2]) for name in df[2].sort_values(by=['MIN'], ascending=False)[:5]]\n",
    "            ben = [cleanName(name, team_player[team2]) for name in df[2].sort_values(by=['MIN'], ascending=False)[5:10]]\n",
    "            team2_players = starters+ben\n",
    "        games_per_season[season].append([team1_players, team2_players, scores])\n",
    "    return games_per_season\n",
    "            \n",
    "def cleanName(name, team_players):\n",
    "    i = len(name) - 1\n",
    "    while i >= 0:\n",
    "        if not name[i].islower():\n",
    "            break\n",
    "    name = name[:int(i/2)]\n",
    "    cleaned = name.split('.')\n",
    "    for player in team_players:\n",
    "        if player.startswith(cleaned[0].strip()) and player.split(' ')[1] == cleaned[1].strip():\n",
    "            return player\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('team_data/team_abrv.pkl', 'rb') as fp:\n",
    "    team_abrv = pickle.load(fp)\n",
    "with open('team_data/team_players.pkl', 'rb') as fp:\n",
    "    team_players = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game_info_processed = process_game_info(test_game_info, team_players, team_abrv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_game_info_processed = process_game_info(train_game_info, team_players, team_abrv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
