{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as url\n",
    "import re\n",
    "import pandas\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract player info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_player_id_per_year(years):\n",
    "    players_season = {}\n",
    "    for year in years:\n",
    "        players = {}\n",
    "        url_year = \"http://www.espn.com/nba/statistics/player/_/stat/scoring-per-game/sort/avgPoints/year/\" + str(year) + \"/qualified/false\"\n",
    "        for page in [-1] + list(range(41, 482, 40)):\n",
    "            url_page = \"\"\n",
    "            if page == -1:\n",
    "                url_page = url_year\n",
    "            else:\n",
    "                url_page = url_year + \"/count/\" + str(page)\n",
    "            try:\n",
    "                html_page = url.urlopen(url_page)\n",
    "                soup = BeautifulSoup(html_page)\n",
    "                for link in soup.findAll('a', attrs={'href': re.compile(\"^http://www.espn.com/nba/player/_/id\")}):\n",
    "                    players_id = link.get('href').split('/')\n",
    "                    players[players_id[len(players_id)-1]] = players_id[len(players_id)-2]\n",
    "            except url.HTTPError:\n",
    "                pass\n",
    "        players_season[str(year-1) + \"-\" + str(year)] = players\n",
    "    return players_season;    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_players(player_id_years, years):\n",
    "    player_info = {}\n",
    "    for year in years:\n",
    "        player_id = player_id_years[str(year-1)+\"-\"+str(year)]\n",
    "        for player in player_id:\n",
    "            url_path = \"http://www.espn.com/nba/player/gamelog/_/id/\" + player_id[player] + \"/year/\" + str(year) + \"/\" + player\n",
    "            try:\n",
    "                dataframe = pandas.read_html(url_path)\n",
    "                if player not in player_info:\n",
    "                    player_info[player] = {}\n",
    "                player_info[player][str(year-1)+\"-\"+str(year)] = dataframe\n",
    "            except url.HTTPError:\n",
    "                pass\n",
    "    return player_info\n",
    "# sample: http://www.espn.com/nba/player/stats/_/id/2994526/bryn-forbes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_data_processing(data):\n",
    "    X = {}\n",
    "    # print (data[player][2017][1][index:index+2])\n",
    "    for player in data:\n",
    "        for year in data[player]:\n",
    "            feature  = []\n",
    "            table = np.array(data[player][year][1][0])\n",
    "            if len(table)<4:\n",
    "                continue\n",
    "            if 'REGULAR SEASON STATS' not in table:\n",
    "                continue\n",
    "            index = int(np.argwhere(table == 'REGULAR SEASON STATS'))\n",
    "            for i in range (1,15):\n",
    "                x= str(data[player][year][1][index+1:index+2][i])\n",
    "                x = x.split('\\n')[0]\n",
    "                x = x.split(' ')[-1]\n",
    "                if '-' in x:\n",
    "                    x = x.split('-')[-1]\n",
    "                feature.append(x)\n",
    "            feature = np.asarray(feature)\n",
    "            feature = list(map(eval, feature))\n",
    "            if player not in X:\n",
    "                X[player]={}\n",
    "            if year not in X[player]:\n",
    "                X[player][year] = []\n",
    "            X[player][year] = feature\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read player data from Internet and store locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player_id_years = find_player_id_per_year([2018])\n",
    "# test_player_info = read_players(player_id_years,[2018])\n",
    "# player_id_years = find_player_id_per_year([2013,2014,2015,2016,2017])\n",
    "# train_player_info = read_players(player_id_years, [2013,2014,2015,2016,2017])\n",
    "# import pickle \n",
    "# with open('player_data/test_player_info.pkl', 'wb') as fp:\n",
    "#     pickle.dump(test_player_info, fp)\n",
    "# with open('player_data/train_player_info.pkl', 'wb') as fp:\n",
    "#     pickle.dump(train_player_info, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read raw player data locally and get them processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('player_data/test_player_info.pkl', 'rb') as fp:\n",
    "#     test_player_info = pickle.load(fp)\n",
    "# with open('player_data/train_player_info.pkl', 'rb') as fp:\n",
    "#     train_player_info = pickle.load(fp)\n",
    "    \n",
    "# test_processed_player_data = player_data_processing(test_player_info)\n",
    "# train_processed_player_data = player_data_processing(train_player_info)\n",
    "\n",
    "# with open('player_data/test_processed_player_data.pkl', 'wb') as fp:\n",
    "#     pickle.dump(test_processed_player_data, fp)\n",
    "# with open('player_data/train_processed_player_data.pkl', 'wb') as fp:\n",
    "#     pickle.dump(train_processed_player_data, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract team info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_teams_url(team_url):\n",
    "    html_page = url.urlopen(team_url)\n",
    "    soup = BeautifulSoup(html_page)\n",
    "    team_url = []\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^http://www.espn.com/nba/team/_/name/\")}):\n",
    "        team_url.append(link.get('href'))\n",
    "    return set(team_url)\n",
    "\n",
    "\n",
    "def read_teams(team_url, years):\n",
    "    team_stat = {}\n",
    "    for url_path in team_url:\n",
    "        url_component = url_path.split('/_/')\n",
    "        stat_url = url_component[0] + '/schedule/_/' + 'name/' + url_component[1].split('/')[1] + '/season/'\n",
    "        year_stat = {}\n",
    "        for year in years:\n",
    "            try:\n",
    "                dataframe = pandas.read_html(stat_url + str(year) + '/seasontype/2')\n",
    "                year_stat[year] = dataframe\n",
    "            except url.HTTPError:\n",
    "                pass\n",
    "            print(stat_url + str(year) + '/seasontype/2')\n",
    "        frames = []\n",
    "        for year in years:\n",
    "            df_changed = year_stat[year][2].loc[2:, 0:2]\n",
    "            before_half = df_changed[0].str.split(\" \",expand=True)[1].isin([\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\"])\n",
    "            df_changed[0][before_half] = df_changed[0][before_half]+', '+str(year)\n",
    "            df_changed[0][~before_half] = df_changed[0][~before_half]+', '+str(year-1)\n",
    "            df_changed[3] = str(year - 1) + \"-\" + str(year)\n",
    "            frames.append(df_changed)\n",
    "        result = pandas.concat(frames)\n",
    "        result.index = range(len(result))\n",
    "        \n",
    "        team_name_component = url_component[1][5:].split('/')[1].split('-')[:-1]\n",
    "        team_name = ' '.join(team_name_component)\n",
    "        \n",
    "        team_stat[team_name] = result\n",
    "    return team_stat\n",
    "\n",
    "def read_teams_total_and_players_per_year(team_url, years):\n",
    "    team_total = {}\n",
    "    for url_path in team_url:\n",
    "        url_component = url_path.split('/_/')\n",
    "        stat_url = url_component[0] + '/stats/_/' + 'name/' + url_component[1].split('/')[1] + '/year/'\n",
    "        year_info = {}\n",
    "        for year in years:\n",
    "            try:\n",
    "                dataframe = pandas.read_html(stat_url + str(year))\n",
    "                team_total_info = pandas.concat([dataframe[0][len(dataframe[0])-1:],dataframe[1][len(dataframe[1])-1:]])\n",
    "                team_total_info.index = range(len(team_total_info))\n",
    "                team_players = dataframe[0][2:][0]\n",
    "                team_players.index = range(len(team_players))\n",
    "                year_info[str(year-1) + \"-\" + str(year)] = (team_total_info, team_players)\n",
    "            except url.HTTPError:\n",
    "                pass\n",
    "            print(stat_url + str(year))\n",
    "        \n",
    "        team_name_component = url_component[1][5:].split('/')[1].split('-')[:-1]\n",
    "        team_name = ' '.join(team_name_component)\n",
    "        \n",
    "        team_total[team_name] = year_info\n",
    "    return team_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "def data_processing(test):\n",
    "    game_info = {}\n",
    "    for team in test:\n",
    "        for i in range(5,len(test[team][1])):\n",
    "            #opponent of the current game\n",
    "            if team not in game_info:\n",
    "                game_info[team] = []\n",
    "            #the results of former 5 games of the team\n",
    "            info = []\n",
    "            #opponent info\n",
    "            opp_info = test[team][1][i].strip().lower()\n",
    "            if opp_info[0] == '@':\n",
    "                opp = opp_info[1:].strip()\n",
    "                info.append(opp)\n",
    "            else:\n",
    "                opp = opp_info[2:].strip()\n",
    "                info.append(opp)\n",
    "            #lose or win\n",
    "            for j in range(1,6):\n",
    "                if test[team][2][5-j][0] == 'L':\n",
    "                    info.append('0')\n",
    "                else:\n",
    "                    info.append('1')\n",
    "            #Home field or away of the current game\n",
    "            if (test[team][1][i].strip()[0] == 'v'):\n",
    "                info.append('1')\n",
    "            else:\n",
    "                info.append('0')     \n",
    "            if (test[team][2][i][0] == 'L'):\n",
    "                game_info[team].append((info, '0', test[team][3][i]))\n",
    "            else:\n",
    "                game_info[team].append((info, '1', test[team][3][i]))\n",
    "                \n",
    "    return game_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_and_players_to_dict(data):\n",
    "    total_dict = {}\n",
    "    team_players = {}\n",
    "    for team in data:\n",
    "        total_dict[team] = {}\n",
    "        team_players[team] = {}\n",
    "        for season in data[team]:\n",
    "            #process team total info first\n",
    "            lst = [data[team][season][0][1][0]]\n",
    "            for i in range(4, 14):\n",
    "                lst.append(str(data[team][season][0][i][0]))\n",
    "            for i in range(1, 15):\n",
    "                lst.append(str(data[team][season][0][i][1]))\n",
    "            total_dict[team][season] = lst\n",
    "            #determine the players of each team per season\n",
    "            team_players[team][season] = [data[team][season][1][i][:data[team][season][1][i].rfind(',')] for i in range(len(data[team][season][1]))]\n",
    "    return total_dict, team_players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read team-related data from Internet and store locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# team_url = find_teams_url(\"http://www.espn.com/nba/teams\")\n",
    "\n",
    "# test = read_teams(team_url, [2018])\n",
    "# train = read_teams(team_url, [2013,2014,2015,2016,2017])\n",
    "# all_games_train = data_processing(train)\n",
    "# all_games_test = data_processing(test)\n",
    "# with open('team_data/all_games_train.pkl', 'wb') as fp:\n",
    "#     pickle.dump(all_games_train, fp)\n",
    "# with open('team_data/all_games_test.pkl', 'wb') as fp:\n",
    "#     pickle.dump(all_games_train, fp)\n",
    "\n",
    "\n",
    "# test_total_and_players = read_teams_total_and_players_per_year(team_url, [2018])\n",
    "# train_total_and_players = read_teams_total_and_players_per_year(team_url, [2013,2014,2015,2016,2017])\n",
    "\n",
    "# test_total, test_team_players = total_and_players_to_dict(test_total_and_players)\n",
    "# train_total, train_team_players = total_and_players_to_dict(train_total_and_players)\n",
    "# with open('team_data/test_total.pkl', 'wb') as fp:\n",
    "#     pickle.dump(test_total, fp)\n",
    "# with open('team_data/train_total.pkl', 'wb') as fp:\n",
    "#     pickle.dump(train_total, fp)\n",
    "\n",
    "# team_players = {}\n",
    "# for team in train_team_players:\n",
    "#     team_players[team] = train_team_players[team]\n",
    "#     for season in test_team_players[team]:\n",
    "#         team_players[team][season] = test_team_players[team][season]\n",
    "# with open('team_data/team_players.pkl', 'wb') as fp:\n",
    "#     pickle.dump(team_players, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract game info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_game_info(team_url, years):\n",
    "    games_url = {}\n",
    "    for url_path in team_url:\n",
    "        url_component = url_path.split('/_/')\n",
    "        stat_url = url_component[0] + '/schedule/_/' + 'name/' + url_component[1].split('/')[1] + '/season/'\n",
    "        for year in years:\n",
    "            if year not in games_url:\n",
    "                games_url[year] = set()\n",
    "            try:\n",
    "                html_page = url.urlopen(stat_url + str(year) + '/seasontype/2')\n",
    "                soup = BeautifulSoup(html_page)\n",
    "                for link in soup.findAll('a', attrs={'href': re.compile(\"^http://www.espn.com/nba/game\")}):\n",
    "                    games_url[year].add(\"http://www.espn.com/nba/boxscore?gameId=\" + link.get('href').split('=')[-1])\n",
    "            except url.HTTPError:\n",
    "                pass\n",
    "    games_info = {}\n",
    "    for year in games_url:\n",
    "        games_info[str(year-1)+\"-\"+str(year)] = []\n",
    "        for url_path in games_url[year]:\n",
    "            try:\n",
    "                dataframe = pandas.read_html(url_path)\n",
    "                games_info[str(year-1)+\"-\"+str(year)].append(dataframe)\n",
    "            except url.HTTPError:\n",
    "                pass\n",
    "        \n",
    "    return games_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_game_info(data, team_player, team_name_brev):\n",
    "    games_per_season = {}\n",
    "    for season in data:\n",
    "        games_per_season[season] = []\n",
    "        for df in data[season]:\n",
    "            scores = tuple(df[0]['T'])\n",
    "            #team1 info\n",
    "            team1 = team_name_brev[df[0]['Unnamed: 0'][0]]\n",
    "            df[1].MIN = pandas.to_numeric(df[1].MIN, errors='coerce')\n",
    "            team1_players = [cleanName(name, team_player[team1]) for name in df[1].sort_values(by=['MIN'], ascending=False)['Starters'][:10]]\n",
    "            #team2 info\n",
    "            team2 = team_name_brev[df[0]['Unnamed: 0'][1]]\n",
    "            df[2].MIN = pandas.to_numeric(df[2].MIN, errors='coerce')\n",
    "            team2_players = [cleanName(name, team_player[team2]) for name in df[2].sort_values(by=['MIN'], ascending=False)['Starters'][:10]]            \n",
    "            games_per_season[season].append([team1_players, team2_players, scores])\n",
    "    return games_per_season\n",
    "            \n",
    "def cleanName(name, team_players):\n",
    "    i = len(name) - 1\n",
    "    while i >= 0:\n",
    "        if not name[i].islower():\n",
    "            break\n",
    "    name = name[:int(i/2)]\n",
    "    \n",
    "    if name == 'J. McAdoo':\n",
    "        return 'James Michael McAdoo'\n",
    "    if name == 'A. Jefferson':\n",
    "        return 'Al Jefferson'\n",
    "    if name == 'E. Murphy':\n",
    "        return 'Erik Jay Murphy'\n",
    "    if name == 'D. Granger':\n",
    "        return 'Danny Granger'\n",
    "    \n",
    "    for player_set in team_players.values():\n",
    "        for player in player_set:\n",
    "            if player == 'Total':\n",
    "                continue\n",
    "            if player.startswith(name[0]) and player[player.find(' ')+1:] == name[name.find(' ') + 1:].strip():\n",
    "                return player\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read game data from Internet and store locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_game_info = read_game_info(team_url, [2018])\n",
    "# train_game_info = read_game_info(team_url, [2013,2014,2015,2016,2017])\n",
    "# with open('game_data/test_game_info_raw.pkl', 'wb') as fp:\n",
    "#     pickle.dump(test_game_info, fp)\n",
    "# with open('game_data/train_game_info_raw.pkl', 'wb') as fp:\n",
    "#     pickle.dump(train_game_info, fp)\n",
    "# #example: http://www.espn.com/nba/boxscore?gameId=400974442"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read raw game data locally and get them processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('team_data/team_abrv.pkl', 'rb') as fp:\n",
    "    team_abrv = pickle.load(fp)\n",
    "with open('team_data/team_players.pkl', 'rb') as fp:\n",
    "    team_players = pickle.load(fp)   \n",
    "team_abrv['WSH'] = 'washington'\n",
    "team_abrv['POR'] = 'portland trail'\n",
    "team_abrv['NY'] = 'new york'\n",
    "team_abrv['BKN'] = 'brooklyn'\n",
    "team_abrv['GS'] = 'golden state'\n",
    "team_abrv['UTAH'] = 'utah'\n",
    "team_abrv['SA'] = 'san antonio'\n",
    "team_abrv['PHX'] = \"phoenix\"\n",
    "team_abrv['LAC'] = 'la'\n",
    "test_game_info_processed = process_game_info(test_game_info, team_players, team_abrv)\n",
    "train_game_info_processed = process_game_info(train_game_info, team_players, team_abrv)\n",
    "with open('game_data/train_game_info_processed.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_game_info_processed, fp)\n",
    "with open('game_data/test_game_info_processed.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_game_info_processed, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle missing player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('player_data/missing.pkl', 'rb') as fp:\n",
    "    missing = pickle.load(fp)\n",
    "player_id_years = find_player_id_per_year([2013,2014,2015,2016,2017])\n",
    "player_id = {}\n",
    "for pi in player_id_years.values():\n",
    "    player_id = {**pi, **player_id} \n",
    "player_info = {}\n",
    "for player_season in missing:\n",
    "    player = player_season[0]\n",
    "    season = player_season[1]\n",
    "    year = season.split('-')[1]\n",
    "    url_path = \"http://www.espn.com/nba/player/gamelog/_/id/\" + player_id[player] + \"/year/\" + year + \"/\" + player\n",
    "    print(url_path)\n",
    "    try:\n",
    "        dataframe = pandas.read_html(url_path)\n",
    "        if player not in player_info:\n",
    "            player_info[player] = {}\n",
    "        player_info[player][season] = dataframe\n",
    "    except url.HTTPError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(player_info['jose-calderon']['2012-2013'][1][0])\n",
    "def player_data_processing(data):\n",
    "    X = {}\n",
    "    # print (data[player][2017][1][index:index+2])\n",
    "    for player in data:\n",
    "        for year in data[player]:\n",
    "            feature  = []\n",
    "            table = np.array(data[player][year][1][0])\n",
    "            if len(table)<4:\n",
    "                continue\n",
    "    #         print (table)\n",
    "            if 'REGULAR SEASON STATS' not in table:\n",
    "                continue\n",
    "            index = int(np.argwhere(table == 'REGULAR SEASON STATS'))\n",
    "    #         print (index)\n",
    "            for i in range (1,15):\n",
    "                x= str(data[player][year][1][index+1:index+2][i])\n",
    "                x = x.split('\\n')[0]\n",
    "                x = x.split(' ')[-1]\n",
    "                if '-' in x:\n",
    "                    x = x.split('-')[-1]\n",
    "                feature.append(x)\n",
    "            feature = np.asarray(feature)\n",
    "            feature = list(map(eval, feature))\n",
    "            if player not in X:\n",
    "                X[player]={}\n",
    "            if year not in X[player]:\n",
    "                X[player][year] = []\n",
    "            X[player][year] = feature\n",
    "    return X\n",
    "missed = player_data_processing(player_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('player_data/train_processed_player_data.pkl', 'rb') as fp:\n",
    "    train_processed_player_data = pickle.load(fp)\n",
    "for player in missed:\n",
    "    for season in missed[player]:\n",
    "        train_processed_player_data[player][season] = missed[player][season]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
